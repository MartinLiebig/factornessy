{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statistical-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "from scipy.stats import norm\n",
    "import plotly.express as px\n",
    "\n",
    "from factor_estimator import *\n",
    "from asset_allocation import AssetAllocation\n",
    "from diversification_analyzer import *\n",
    "from index_data_handler import IndexDataHandler\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "register_matplotlib_converters()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-theorem",
   "metadata": {},
   "source": [
    "Factor investing is one of the most interesting approaches to \"outperform the market\" if the market is a usual market cap weighted portfolio (like MSCI World, MSCI ACWI, FTSE All World or to a degree a S&P500).\n",
    "When diving into the literature one finds a whole host of factors which may be valuable. The most intersting ones come from Fama and French (Size (SmB), Value (HmL), Conservative (CvA) and Robustness (RmW). There are more out there like Momentum (Carhart, 1997) or Liquidity (Pástor, Stambaugh 2003).\n",
    "Since the author of this analysis is based out of Germany, the amount of different investment options is limited. Thus we keep with the following selection, where the number represents the MSCI Index code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "referenced-brother",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'High Dividend (Europe)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-58d091b9f4a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mindex_codes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Small-Cap\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mindex_codes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"High Dividend\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mindex_codes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"High Dividend (Europe)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_codes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'High Dividend (Europe)'"
     ]
    }
   ],
   "source": [
    "start_date=\"19981130\"\n",
    "indexHandler = IndexDataHandler(start_date=start_date)\n",
    "index_codes = {**indexHandler.get_available_indices(\"Developed\"), **indexHandler.get_available_indices(\"US\")}\n",
    "index_codes.pop(\"Small-Cap\")\n",
    "index_codes.pop(\"High Dividend\")\n",
    "display(pd.DataFrame.from_dict(index_codes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-washer",
   "metadata": {},
   "source": [
    "The question asked in this analysis is: **How do I mix the ETFs for the indecies** above?\n",
    "\n",
    "In general there are two approaches for factor investing. The integrated approach uses one etf where the stocks are picked to satisfy more then one factor at a time. Those ETFs are a one-stop solution, but have significantly higher costs. Also there is currently only a very limited selection available in Germany. The most important one is [iShares Edge MSCI World Multifactor](https://www.justetf.com/de-en/etf-profile.html?isin=IE00BZ0PKT83) which is based on the WORLD DIVERSIFIED MULTI-FACTOR index.\n",
    "The TER for this is 0.5% and thus its ~0.35% higher than a market neutral MSCI World and ~0.25% higher than most 'single factor ETF'\n",
    "\n",
    "The other approach is a *index of indicies* where you mix different ETFs to target more than a single factor. The reasoning behind that is, that factors are not stable over time and it is desireable to target more than one for continuity.\n",
    "\n",
    "In this analysis we will try to estimate the 5 factors of the Fama French 5-Factor model. Those are MKT, SMB, HML, CWA and RMW. We then try to find the most suitable combination of ETFs for our purposes.\n",
    "\n",
    "Let's get the data first. The data is directly taking from MSCI, and is thus available for a longer range. We examine data from 1998-2020, since this is the time span with daily data available. Every point is always normalized to the 01/01/1997, so we can look at it like we invested a euro back then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "reload=False\n",
    "#start_date=\"20080101\"\n",
    "for key in index_codes.keys():\n",
    "    code = index_codes[key][\"code\"]\n",
    "    print(\"reading \",key)\n",
    "    d = indexHandler.get_historic_stock_data(code, reload=reload)\n",
    "    d.rename(columns={\"level_eod\":key}, inplace=True)\n",
    "    data.append(d)\n",
    "\n",
    "df = data[0]\n",
    "for i in range(1,len(data)):\n",
    "    df = df.merge(data[i], how='left',left_index=True,right_index=True)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-productivity",
   "metadata": {},
   "source": [
    "To check the data and to understand, let's chart the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "plt.figure(figsize=(20, 12))\n",
    "for k in index_codes.keys():\n",
    "\n",
    "    plt.plot(df[\"date\"],df[k], label=k)\n",
    "plt.legend(prop={'size':24})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-celebrity",
   "metadata": {},
   "source": [
    "We can see that all factors outperform the MSCI World.\n",
    "We can also see that the two best running factors are Momentum and Small-Cap (Value).\n",
    "\n",
    "Lets now calculate the factors for each of our indicies. The idea is, that the change of market price w.r.t to the risk free return can be descriped as \n",
    "\\begin{equation}\n",
    "    r_f = a \\cdot MKT+b \\cdot SMB +c \\cdot HML+d \\cdot RMW+e \\cdot CMA\n",
    "\\end{equation}\n",
    "The original Fama French paper knew a,b,c,d and e for a given portfolio (those are metrics like P/E), and deterimed the influence factors.\n",
    "We do it the other way around. We know the influence factors from the paper and want to determine the 'sizeness' of an index (so we determine a,b,c,d,e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = {}\n",
    "fit_results = {}\n",
    "for key in index_codes.keys():\n",
    "    code = index_codes[key][\"code\"]\n",
    "    print(\"calculating for \",code, key)\n",
    "    fitted_factors, results, data = estimate(code,start_date=start_date,region=index_codes[key][\"region\"])\n",
    "\n",
    "    factors[key] = fitted_factors\n",
    "    fit_results[key] = results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-charter",
   "metadata": {},
   "source": [
    "Before we move forward, lets have a look at the quality of the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "D={}\n",
    "plt.figure(figsize=(12, 5))\n",
    "for key in factors.keys():\n",
    "    D[key] = fit_results[key].rsquared\n",
    "plt.bar(*zip(*D.items()))\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-gossip",
   "metadata": {},
   "source": [
    "From my perspective the r-squared value is relativily low.. The values of 0.4 are rather weak and maybe we should run further analysis here?\n",
    "The original paper also quotes relativly low r² values. Maybe that's just normal?\n",
    "\n",
    "We can also have a quick look at the global factorness of each index. Factorness is defined as\n",
    "\\begin{equation}\n",
    "\\phi = \\sum_i f_i\n",
    "\\end{equation}\n",
    "where $f_i$ is the individual factor. The key question of this analysis is if $\\phi$ is the factor to optimize on or how to weight the individual contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "D={}\n",
    "plt.figure(figsize=(12, 5))\n",
    "for key in factors.keys():\n",
    "    D[key] = factors[key].sum()\n",
    "plt.bar(*zip(*D.items()))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-mention",
   "metadata": {},
   "source": [
    "What we can see here is kind of telling. The Multi-Factor index is not outstanding! If the Mult-Factor index is clearly surperior we would have expected a high value here.\n",
    "Let's look into the details and the individual factor for each of the 5 factors of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5)\n",
    "i = 0\n",
    "\n",
    "for factorname in [\"MKT\",\"SMB\",\"HML\",\"RMW\",\"CMA\"]:\n",
    "    D = {}\n",
    "    for key in factors.keys():\n",
    "        D[key] = factors[key][factorname]\n",
    "    axs[i].bar(*zip(*D.items()))\n",
    "    axs[i].set_title(factorname)\n",
    "    #axs[i].set_ylim(-0.2,0.5)\n",
    "    axs[i].set_ylabel(\"factor coefficent\")\n",
    "    i+=1\n",
    "fig.set_size_inches(18.5, 18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-profit",
   "metadata": {},
   "source": [
    "And for completness, lets look at it the other way around and see it grouped by the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(index_codes))\n",
    "\n",
    "width = 0.35  # the width of the bars\n",
    "i = 0\n",
    "for key in factors.keys():\n",
    "    D = {}\n",
    "    for factorname in [\"MKT\",\"SMB\",\"HML\",\"RMW\",\"CMA\"]:\n",
    "        D[factorname] = factors[key][factorname]\n",
    "        \n",
    "    ax[i].bar(*zip(*D.items()),label=key)\n",
    "    ax[i].set_title(key)\n",
    "    #ax[i].set_ylim(-0.2,0.5)\n",
    "    ax[i].set_ylabel(\"factor coefficent\")\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(10, 30)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-federation",
   "metadata": {},
   "source": [
    "What can we learn from here? First: Small Cap (Value) seams to be a beast! It offers exposure to the market, SMB and HML! And it does not even have negative contributions to RMW and CMA. Wow! One needs to remind oneselve, that Small Cap (Value) only has a [single ETF](https://www.justetf.com/de-en/etf-profile.html?groupField=index&from=search&isin=IE00BSPLC413) in Germany which is relativly expensive (0.3% TER)\n",
    "\n",
    "# Factor Exposure and Expected Returns for Mixtures\n",
    "\n",
    "Lets move into the optimization. We first define our factor matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_matrix = pd.DataFrame.from_dict(factors).drop([\"Intercept\"],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-campbell",
   "metadata": {},
   "source": [
    "The factorness vector $\\vec{f}$ can now be expressed as:\n",
    "\\begin{equation}\n",
    "\\vec{f} = \\underline{F} \\cdot \\alpha\n",
    "\\end{equation}\n",
    "Where $\\underline{F}$ is the factor matrix and $\\alpha$ our asset allocation vector.\n",
    "\n",
    "**This boils down to the key open question: How to weight the different factors?** \n",
    "\n",
    "As an idea, lets take the average factor values from FF, scale them to a year and take them as a weightening factor. We can then just calculate the returns as:\n",
    "\\begin{equation}\n",
    "    r = \\vec{f} \\cdot \\vec{P}\n",
    "\\end{equation}\n",
    "Where P holds the averages of the factors.\n",
    "\n",
    "*Note: Maybe we want to take the monthly factor values here?*\n",
    "\n",
    "*Note2: Just scaling it ignores the compound effect*\n",
    "\n",
    "*Note3: The Small Cap Value is like a US ETF, so ideally one may want to use the higher US values here?*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_names = df.columns.tolist()\n",
    "index_names.remove(\"date\")\n",
    "numberOfIndecies = len(index_names)\n",
    "def factorness(alpha):\n",
    "  return factor_matrix.dot(alpha)\n",
    "\n",
    "average_factors = get_average_factors(\"Developed\")*252\n",
    "def excess_returns(alpha):\n",
    "    factors = factorness(alpha)\n",
    "    returns = average_factors.dot(factors)\n",
    "    return returns*100\n",
    "asset_allocations = {} # this will hold all proposals for later loops etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-sacrifice",
   "metadata": {},
   "source": [
    "Lets now define a few portfolios which can be interesting for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asset_allocation import AssetAllocation\n",
    "\n",
    "asset_allocations[\"Pure Multi\"]= AssetAllocation(index_names).set_allocation_by_dict({\"Multi-Factor\":1})\n",
    "asset_allocations[\"Pure World\"]= AssetAllocation(index_names).set_allocation_by_dict({\"MSCI World\":1})\n",
    "asset_allocations[\"Small-Value-Momentum\"] = AssetAllocation(index_names).set_allocation_by_dict({\"Small-Cap (Value)\":0.4,\"Momentum\":0.2,\"Value\":0.4})\n",
    "asset_allocations[\"Small-Momentum-Value\"] = AssetAllocation(index_names).set_allocation_by_dict({\"Small-Cap (Value)\":0.4,\"Momentum\":0.4,\"Value\":0.2})\n",
    "asset_allocations[\"Small-Quality-Value\"] = AssetAllocation(index_names).set_allocation_by_dict({\"Small-Cap (Value)\":0.4,\"Quality\":0.2,\"Value\":0.4})\n",
    "asset_allocations[\"Even Allocation\"] = AssetAllocation(index_names).set_allocation_by_dict({\"Value\":0.2,\"Quality\":0.2,\"Momentum\":0.2,\"Low Volatility\":0.2,\"Small-Cap (Value)\":0.2})\n",
    "asset_allocations[\"Momentum Tilt\"] = AssetAllocation(index_names).set_allocation_by_dict({\"MSCI World\":0.6,\"Momentum\":0.4})\n",
    "asset_allocations[\"Value Tilt\"] = AssetAllocation(index_names).set_allocation_by_dict({\"MSCI World\":0.6,\"Value\":0.4})\n",
    "asset_allocations[\"Even (No Vola)\"] = AssetAllocation(index_names).set_allocation_by_dict({\"Value\":0.25,\"Quality\":0.25,\"Momentum\":0.25,\"Small-Cap (Value)\":0.25})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-remains",
   "metadata": {},
   "source": [
    "We created a few things. As the raw baseline we create the Pure World one, which is just the MSCI World. Then we have a pure Multi which is just the Multi factor one.\n",
    "\n",
    "We define a few mixtures, an even allocation between the 5 factors as well as simple value/momentum tilted portfolios which have 60% MSCI world and 40% of the respective factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "#print(asset_allocations.keys())\n",
    "for key in asset_allocations.keys():\n",
    "    result_dict[key] = {}\n",
    "    factorness_vector = factorness(asset_allocations[key].to_array())\n",
    "    result_dict[key][\"factorness\"] = float(factorness_vector.sum())\n",
    "    result_dict[key][\"returns\"] = float(excess_returns(asset_allocations[key].to_array())[0])\n",
    "    for name, value in zip([\"MKT\",\"SMB\",\"HML\",\"RMW\",\"CMA\"],factorness_vector[0]):\n",
    "        result_dict[key][name] = value\n",
    "\n",
    "    \n",
    "result_df = pd.DataFrame(result_dict).T.sort_values(\"returns\",ascending=False)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-peninsula",
   "metadata": {},
   "source": [
    "# Numerical Optimization for Returns\n",
    "We can now try to use a numerical optimization in order to find the result which maximizes the total factorness $\\phi$. One key thing here is, that we need to specifiy bonds. If not the optimizer will throw everything into Small Cap (Value), since it's a beast.\n",
    "Lets define bounds as \\[0, 0.4\\] and lets also deactivate the use of multi factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "# objective function\n",
    "def func(alpha):\n",
    "    return -1*excess_returns(alpha)\n",
    "\n",
    "# constraint: sum(weights) = 1\n",
    "fconst = lambda w: 1 - sum(w)\n",
    "cons   = ({'type':'eq','fun':fconst})\n",
    "\n",
    "# initial weights\n",
    "w0 =  np.ones((len(index_codes),1))/len(index_codes)\n",
    "\n",
    "# define bounds\n",
    "b    = (0.0, 0.4) \n",
    "bnds = [b for i in range(0,numberOfIndecies)]\n",
    "bnds[3] = (0.0,0.0001)\n",
    "# minimize\n",
    "sol_returns  = minimize(func,\n",
    "                w0,\n",
    "                bounds      = bnds,\n",
    "                constraints = cons)\n",
    "print(sol_returns.fun*-1)\n",
    "return_optimize_dict = {}\n",
    "for alloc, name in zip(sol_returns.x,factor_matrix.columns):\n",
    "    return_optimize_dict[name]=alloc\n",
    "\n",
    "display(pd.DataFrame.from_records(return_optimize_dict,index=[\"allocation\"]).round(decimals=2))\n",
    "asset_allocations[\"Return Optimized\"] = AssetAllocation(index_names).set_allocations(sol_returns.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-lighting",
   "metadata": {},
   "source": [
    "\n",
    "It is somewhat funny to see, that the result of this longer analys\n",
    "is is a bit the trivial solution of taking the 3 best running ones and mixing them. The reason for this is that we do not yet consider volatility nor diversification as a constraint for our optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict = {}\n",
    "for key, value in asset_allocations.items():\n",
    "    return_dict[key] = float(excess_returns(value.to_array()).iloc[0])\n",
    "# normalize to MSCI World returns\n",
    "pure_world_return = return_dict[\"Pure World\"]\n",
    "for key, value in return_dict.items():\n",
    "    return_dict[key] = value-pure_world_return\n",
    "    \n",
    "display(pd.DataFrame.from_records(return_dict,index=[\"returns\"]))\n",
    "plt.bar(return_dict.keys(), return_dict.values())\n",
    "plt.title(\"Excess Returns over MSCI World\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_estimator import create_index_of_indices\n",
    "for key, value in asset_allocations.items():\n",
    "    df = create_index_of_indices(df,key,value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "allocs_to_show = [key for key in asset_allocations.keys()]\n",
    "allocs_to_show.remove(\"Pure World\")\n",
    "allocs_to_show.append(\"Momentum\")\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for k in allocs_to_show:\n",
    "    plt.plot(df[\"date\"],df[k], label=k)\n",
    "#gfg = sns.lineplot(data = df[c])\n",
    "plt.legend(prop={'size':24})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-relations",
   "metadata": {},
   "source": [
    "# Taking Volatility into Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def volatility(alpha):\n",
    "    alloc = AssetAllocation(index_names)\n",
    "    alloc.set_allocations(alpha)\n",
    "    aux = create_index_of_indices(df,\"aux\",alloc)\n",
    "    std_dev = aux[\"aux\"].pct_change().dropna().std()*100\n",
    "    return std_dev\n",
    "\n",
    "\n",
    "volatility_dict = {}\n",
    "for key in asset_allocations.keys():\n",
    "    volatility_dict[key] = volatility(asset_allocations[key].to_array())#df[key].pct_change().dropna().std()*100\n",
    "display(pd.DataFrame.from_records(volatility_dict,index=[\"daily standard deviation\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "# objective function\n",
    "regularizationFactor = 1\n",
    "volatility_chart_dict = {}\n",
    "\n",
    "def func(alpha):\n",
    "    return -1*excess_returns(alpha).sum()+regularizationFactor*volatility(alpha)\n",
    "results = {}\n",
    "for regularizationFactor in np.arange(0,15,1):\n",
    "    \n",
    "\n",
    "\n",
    "    # constraint: sum(weights) = 1\n",
    "    fconst = lambda w: 1 - sum(w)\n",
    "    cons   = ({'type':'eq','fun':fconst})\n",
    "\n",
    "    # initial weights\n",
    "    w0 =  np.ones((len(index_codes),1))/len(index_codes)\n",
    "\n",
    "    # define bounds\n",
    "    b    = (0.0, .4) \n",
    "    bnds = [b for i in range(0,numberOfIndecies)]\n",
    "    bnds[3] = (0.0,0.0001)\n",
    "    # minimize\n",
    "    sol  = minimize(func,\n",
    "                    w0,\n",
    "                    bounds      = bnds,\n",
    "                    constraints = cons)\n",
    "\n",
    "    optimized_allocation = AssetAllocation(index_names)\n",
    "    optimized_allocation = optimized_allocation.set_allocations(sol.x)\n",
    "#     print(\"### \"+str(regularizationFactor)+\" ###\")\n",
    "#     print(excess_returns(sol.x).sum(),\"  /  \",volatility(sol.x))\n",
    "#     print(optimized_allocation)\n",
    "    results[regularizationFactor] = optimized_allocation.allocations\n",
    "    results[regularizationFactor][\"Returns\"] = excess_returns(sol.x).sum()\n",
    "    results[regularizationFactor][\"Volatility\"] = volatility(sol.x)\n",
    "\n",
    "fit_results = pd.DataFrame.from_dict(results)\n",
    "for c in fit_results.columns:\n",
    "    fit_results[c] = fit_results[c].round(decimals=2)\n",
    "display(fit_results)\n",
    "\n",
    "# Add the volatility optimized allocation to our main data frame\n",
    "volatility_allocation = AssetAllocation(index_names)\n",
    "for name, alloc in results[10].items():\n",
    "    if(name != \"Returns\" and name != \"Volatility\"):\n",
    "        volatility_allocation.set_allocation(name, alloc)\n",
    "\n",
    "asset_allocations[\"Volatility Optimized\"] = volatility_allocation\n",
    "df = create_index_of_indices(df,\"Volatility Optimized\",volatility_allocation)\n",
    "\n",
    "comparison_dict = {\"Pure World\":{},\"Pure Multi\":{}}\n",
    "comparison_dict[\"Pure World\"][\"Returns\"] = float(excess_returns(asset_allocations[\"Pure World\"].to_array()).sum())\n",
    "comparison_dict[\"Pure World\"][\"Volatility\"] = float(volatility(asset_allocations[\"Pure World\"].to_array()))\n",
    "comparison_dict[\"Pure Multi\"][\"Returns\"] = float(excess_returns(asset_allocations[\"Pure Multi\"].to_array()).sum())\n",
    "comparison_dict[\"Pure Multi\"][\"Volatility\"] = float(volatility(asset_allocations[\"Pure Multi\"].to_array()))\n",
    "comparison_df = pd.DataFrame.from_dict(comparison_dict)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-colon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_df = fit_results.T[[\"Returns\",\"Volatility\"]]\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.title(\"Volatility Analysis\",fontsize=30)\n",
    "plt.xlabel(\"Volatility\",fontsize=24)\n",
    "plt.ylabel(\"Returns\",fontsize=24)\n",
    "plt.grid(True)\n",
    "markersize=12\n",
    "plt.plot(plot_df[\"Volatility\"], plot_df[\"Returns\"],'bo',label=\"optimization\",markersize=markersize)\n",
    "\n",
    "for key, alloc in asset_allocations.items():\n",
    "    if(key != \"Small-Value-Momentum\"):\n",
    "        plt.plot(volatility(alloc.to_array()), excess_returns(alloc.to_array()).sum(),\"o\",label=key,markersize=markersize)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-petroleum",
   "metadata": {},
   "source": [
    "What can we learn from here? Well, we have a few interesting portfolios i think:\n",
    "\n",
    "## The High Risk High Return Portfolio:\n",
    "Thats the top right. It is\n",
    "* 40% Small Cap (Value)\n",
    "* 40% Value\n",
    "* 20% Momentum\n",
    "This gives you roughly 7.35% returns or 1.6% gain over the MSCI World. You pay for it with higher Volatility. \n",
    "\n",
    "## The Low Volatility\n",
    "Thats on the lower left (there are a few, thats the \"10\")::\n",
    "* 33% Low Volatility\n",
    "* 10% Momentum\n",
    "* 32% Quality\n",
    "* 18% Value\n",
    "\n",
    "You can get a roughly 6.5% return (remember, this is compared to US Treasury bonds). Which is 0.7% gain over the MSCI World or roughly the same as the Multi-Factor. \n",
    "\n",
    "# Looking into Volatility\n",
    "\n",
    "For now, we will focus on the 'High Rish High Return Portfolio\". How well did it run for the past years? Those cummulated plots above are nice to look at, because you mostly on top of the MSCI World curve, but it hides a bit the variability of the factor. Lets there fore look at the comparison to the MSCI World. When would we be worse or better off than the MSCI World?\n",
    "To do this we have a look at:\n",
    "\\begin{equation}\n",
    " r = MA(\\%\\text{Change}(\\text{daily_returns}))_{x} -MA(\\%\\text{Change}(\\text{daily_returns}))_{MSCI World}\n",
    "\\end{equation}\n",
    "Where MA is the moving average and %Change is the percent change to the day before.\n",
    "\n",
    "Let's first have a look at the charts for the 'normal Factors' and then for our allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "allocs_to_show = []\n",
    "allocs_to_show.append(\"Momentum\")\n",
    "allocs_to_show.append(\"MSCI World\")\n",
    "allocs_to_show.append(\"Value\")\n",
    "allocs_to_show.append(\"Quality\")\n",
    "plt.figure(figsize=(20, 6))\n",
    "window_size = 60\n",
    "for k in allocs_to_show:\n",
    "    plt.plot(df['date'],df[k].pct_change().rolling(window_size).mean()-df[\"MSCI World\"].pct_change().rolling(window_size).mean(),label=k)\n",
    "plt.legend(prop={'size':12})\n",
    "plt.xlabel('date', fontsize=18)\n",
    "plt.ylabel('Moving avg of % change - Moving avg of % change of MSCI World', fontsize=18)\n",
    "plt.title(\"Comparison of selected allocations to MSCI World\",fontsize=32)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "allocs_to_show = [key for key in asset_allocations.keys()]\n",
    "\n",
    "allocs_to_show.remove(\"Even Allocation\")\n",
    "allocs_to_show.remove(\"Small-Quality-Value\")\n",
    "allocs_to_show.remove(\"Return Optimized\")\n",
    "allocs_to_show.remove(\"Small-Momentum-Value\")\n",
    "#allocs_to_show = [\"MSCI World\",\"Momentum Tilt\"]\n",
    "plt.figure(figsize=(20, 6))\n",
    "window_size = 90\n",
    "for k in allocs_to_show:\n",
    "    plt.plot(df['date'],df[k].pct_change().rolling(window_size).mean()-df[\"MSCI World\"].pct_change().rolling(window_size).mean(),label=k)\n",
    "plt.legend(prop={'size':12})\n",
    "plt.xlabel('date', fontsize=18)\n",
    "plt.ylabel('Moving avg of % change - Moving avg of % change of MSCI World', fontsize=18)\n",
    "plt.title(\"Comparison of selected allocations to MSCI World\",fontsize=32)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-daisy",
   "metadata": {},
   "source": [
    "What we can learn from here, is that there will be always times where a given factor under performs the market. This is totally normal and it *has* to happen. Why? For more returns we get more volatility. More volatility means we are sometimes below the MSCI World, which is less volatile. Thats the point!\n",
    "\n",
    "Lets look at this volatility more on a pratical level. The first two weeks after I bought my first ETF I was checking my stock app hourly. What a blockbuster!\n",
    "This was a while ago and now I act more - passive. But still, most people aren't blind for the short term volatility of their stocks. Before we check the volatility defined as the daily standard deviation.\n",
    "Now let's check the monthly standard deviation in comparison to the MSCI World. How often can we beat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_index_handler = IndexDataHandler(start_date=start_date,frequency=\"END_OF_MONTH\",normalize=True)\n",
    "\n",
    "monthly_data = []\n",
    "\n",
    "#start_date=\"20080101\"\n",
    "for key in index_codes.keys():\n",
    "    code = index_codes[key][\"code\"]\n",
    "    print(\"reading\", key)\n",
    "    d = monthly_index_handler.get_historic_stock_data(code)\n",
    "    d.rename(columns={\"level_eod\":key}, inplace=True)\n",
    "    d = d.dropna()\n",
    "    monthly_data.append(d)\n",
    "\n",
    "monthly_df = monthly_data[0]\n",
    "for i in range(1,len(monthly_data)):\n",
    "    monthly_df = monthly_df.merge(monthly_data[i], how='left',left_index=True,right_index=True)\n",
    "monthly_df = monthly_df.drop(monthly_df.filter(regex='_y$').columns.tolist(),axis=1)\n",
    "\n",
    "\n",
    "for key, value in asset_allocations.items():\n",
    "    monthly_df = create_index_of_indices(monthly_df,key,value)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_dict = {}\n",
    "for key in asset_allocations.keys():\n",
    "    normalized_change_data = monthly_df[key].pct_change()-monthly_df[\"MSCI World\"].pct_change()\n",
    "    normalized_change_data = normalized_change_data.dropna()\n",
    "    normalized_change_data = normalized_change_data*100\n",
    "    mu = normalized_change_data.mean()\n",
    "\n",
    "    histo_dict[key] ={}\n",
    "    histo_dict[key][\"mu\"] = normalized_change_data.mean()\n",
    "    histo_dict[key][\"sigma\"] = normalized_change_data.std()\n",
    "    histo_dict[key][\"kurtosis\"] = normalized_change_data.kurtosis()\n",
    "    histo_dict[key][\"Values>0 [%]\"] = normalized_change_data[normalized_change_data>0].size/normalized_change_data.size*100\n",
    "    if(key==\"Small-Value-Momentum\"):\n",
    "        plt.title(\"Small-Value-Momentum\")\n",
    "        plt.xlabel(\"Percent Gain over MSCI World per Month\")\n",
    "        plt.hist(normalized_change_data,bins=30)\n",
    "plt.show()\n",
    "hist_df = pd.DataFrame.from_dict(histo_dict)\n",
    "display(hist_df)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-device",
   "metadata": {},
   "source": [
    "The mean of the distribution is not too surprising. Our favourite of \"Small-Value-Momentum\" is in average 0.33\\% a month above MSCI World. But this is only _in average_ . In 45% of the cases the MSCI World runs better in a given month. This is partly caused by the kurtosis, which is != 0 for all our ideas.\n",
    "\n",
    "So this means? If you check a month and compare yourself with the MSCI World it tells you nothing. It's literally a coin flip. You need to get sample size to see that its works - which is a somewhat not surprising result - but good that we checked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-sleep",
   "metadata": {},
   "source": [
    "## Value and Volatility Lessons\n",
    "\n",
    "\n",
    "If you look at the Value premium in our first chart you can see that it had a hard time in the time after 2016. Its very well known that Value investing was problematic over the past years. Is that a reason to abondon value?\n",
    "\n",
    "Fama and French show in their 2018 paper [Volatility Lessons](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3081101), that it is totally normal that a factor under performs for 5 years. Their analysis, where there bootstrap themselves a lot of pseudo-dataset, they showed that a 5 year underperformance of a factor is nothing rare. Even a 10 year underperformance is not impossible.\n",
    "\n",
    "Another great paper on the topic is [Reports of Value’s Death May Be Greatly Exaggerated](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3488748). The authors come to the same conclusion as FF above, saying that the recent 'slump' is a strong statistical abnormally, but in 2.5% of their bootstrapped samples they expirienced a similar or worse situation.\n",
    "The authors also show, that most of the arguments by 'Death-Callers' are easily refutable. They however say, that P/E is a bad ratio to start with and that there are some problems with high investments into R&D, which are likely not covered by earnings.\n",
    "\n",
    "We will always see 'finance pornographs' call a factor dead. But as factor investors we need to make ourselves aware, that (to the best of my knowledge), there is no statistical evidence for this. This is in my opinion the pain you need to go throw, to get the better returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-treasure",
   "metadata": {},
   "source": [
    "# The Momentum Value Trade Off\n",
    "\n",
    "Our analysis showed, that it may be very intersting to invest into momentum and value at the same time. After doing the analysis I saw the paper [Value and Momentum everywhere](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1363476), which supports this idea. I would definitly advise you to read it, here are two short quotes from the conclusion:\n",
    "\n",
    "*Value and momentum deliver positive expected abnormal returns in a variety of markets and  asset  classes,  their  combination  performs  even  better  than  either  alone,  and  the  benefits of diversification across markets and asset classes are large*\n",
    "\n",
    "and\n",
    "\n",
    "*We find that value (and momentum) strategies are positively related across markets and asset  classes  and  that  value  and  momentum  are  negatively  related  within  and  across  markets  and  asset  classes*\n",
    "\n",
    "The paper also shows a negative correlation between momentum and value (see Table 1), which is desirable. If two factors are negativly correlated with one another, then they cover for the other when the other is week. Thus you get a more steady total return curve and hopefully it is then easier to stay invested.\n",
    "\n",
    "# 20% Momentum? Or more?\n",
    "\n",
    "So, we setteled down to a portfolio with 40% Small Cap (Value) and the rest is Momentum and Value. The optimization yielded 40% Value and 20% Momentum.\n",
    "The question is: Is that correct?\n",
    "\n",
    "Lets therefore look at two charts.\n",
    "* 1. The raw delta between the 40% Value and other allocations\n",
    "* 2. The same metric we used enough, the moving average in change of daily gains in percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_df = df[[\"date\",\"Small-Value-Momentum\"]]\n",
    "\n",
    "m_alloc = AssetAllocation(index_names)\n",
    "m_alloc.set_allocation(\"Small-Cap (Value)\",0.4)\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(20, 15)\n",
    "window_size = 90\n",
    "for value_alloc in [0.1,0.2,0.3,0.4,0.5]:\n",
    "    m_alloc.set_allocation(\"Value\",value_alloc)\n",
    "    m_alloc.set_allocation(\"Momentum\",0.6-value_alloc)\n",
    "    name = \"Value: \"+str(value_alloc)\n",
    "\n",
    "    momentum_df.loc[:,name] = create_index_of_indices(df,name,m_alloc).loc[:,name]\n",
    "    momentum_df[name+\"_relative\"] =  momentum_df[name].pct_change().rolling(window_size).mean() - df[\"Small-Value-Momentum\"].pct_change().rolling(window_size).mean()\n",
    "\n",
    "    ax[0].plot(momentum_df['date'],momentum_df[name]-df[\"Small-Value-Momentum\"],label=name)\n",
    "    ax[1].plot(momentum_df['date'],momentum_df[name+\"_relative\"],label=name)\n",
    "ax[0].set_title(\"Different Value Ratios compared to 40% Value\")\n",
    "ax[1].set_title(\"Yearly Moving Average of percent change compared to 40% Value\")\n",
    "plt.legend(prop={'size':24})#\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-party",
   "metadata": {},
   "source": [
    "What can we learn from here? For a long long time it would hav eeen beneficial to go in with as much value as possible. Only in 2018 forward Momentum 'kicked back' and is now, even in total, on top.\n",
    "The problem I see here is the sample bias problem. In the end we only work on 22 years of data. If - by accident - the first 15 of them where strong value years our strategy is directled towards more value then it would be good over all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-acrylic",
   "metadata": {},
   "source": [
    "# Holding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "holding_indices = index_codes\n",
    "data = []\n",
    "index_names = [ key for key in holding_indices.keys()]\n",
    "spdr = None\n",
    "for key in holding_indices:\n",
    "    if(holding_indices[key][\"vendor\"] == \"Xtrackers\"):\n",
    "        data.append(get_xtrackers(holding_indices[key][\"ISIN\"],key))\n",
    "    if(holding_indices[key][\"vendor\"] == \"SPDR\"):\n",
    "        data.append(get_spdr(holding_indices[key][\"ticker\"],key))\n",
    "    if (holding_indices[key][\"vendor\"] == \"iShares\"):\n",
    "        #print(\"Will not read \",key,\"iShares is annoying\")\n",
    "        #does not work yet, isharse names INTEL as INTEL Corp etc -.-\n",
    "        data.append(get_iShares(key,key))\n",
    "        \n",
    "\n",
    "holding_df = data[0]\n",
    "for i in range(1,len(data)):\n",
    "    #print(data[i])\n",
    "    holding_df = holding_df.merge(data[i], how='outer',left_on=\"ISIN\",right_on=\"ISIN\",suffixes=('', '_DROP'))\n",
    "    holding_df[\"Name\"].fillna(holding_df[\"Name_DROP\"], inplace=True)\n",
    "    holding_df[\"Country\"].fillna(holding_df[\"Country_DROP\"], inplace=True)\n",
    "    holding_df[\"Industry Classification\"].fillna(holding_df[\"Industry Classification_DROP\"], inplace=True)\n",
    "    holding_df = holding_df.fillna(0).filter(regex='^(?!.*_DROP)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-webmaster",
   "metadata": {},
   "source": [
    "## Top Holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_allocation = AssetAllocation(index_names)\n",
    "\n",
    "chosen_allocation.set_allocation(\"Momentum\",0.2)\n",
    "chosen_allocation.set_allocation(\"Value\",0.4)\n",
    "chosen_allocation.set_allocation(\"Small-Cap (Value)\",0.4)\n",
    "holding_df = create_index_of_indices(holding_df, \"Small-Value-Momentum\", chosen_allocation)\n",
    "\n",
    "\n",
    "holding_df = holding_df.sort_values(\"Small-Value-Momentum\",ascending=False)\n",
    "#display(holding_df[[\"Name\",\"Small-Value-Momentum\"]].head(50))\n",
    "\n",
    "fig = px.bar(holding_df.head(30), x='Name', y='Small-Value-Momentum')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = holding_df.groupby(\"Industry Classification\").sum()\n",
    "grouped = grouped.reset_index()\n",
    "grouped[\"Industry Classification\"][grouped[\"Small-Value-Momentum\"] <0.01] = \"Other\"\n",
    "#print(grouped)\n",
    "fig = px.pie(grouped, values='Small-Value-Momentum', names='Industry Classification')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = holding_df.groupby(\"Country\").sum()\n",
    "grouped = grouped.reset_index()\n",
    "grouped[\"Country\"].loc[grouped[\"Small-Value-Momentum\"] <0.01] = \"Other\"\n",
    "#print(grouped)\n",
    "fig = px.pie(grouped, values='Small-Value-Momentum', names='Country')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-supervision",
   "metadata": {},
   "source": [
    "# The European Question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-right",
   "metadata": {},
   "source": [
    "From the holding analysis above we learned, that  the current portfolio is tilted towards the US by 70%. As a comparison - the normal MSCI World has a ~66% US stocks.\n",
    "One may ask the question if it makes sense to enhance the european part of the portfolio to get more independend to the US. We can quickly examine historical performance of the corresponding European indices for Momentum, Value and Small Cap Value:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-immunology",
   "metadata": {},
   "source": [
    "european_indices = indexHandler.get_available_indices(region=\"Europe\")\n",
    "em_indices = indexHandler.get_available_indices(region=\"EM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "european_indices = indexHandler.get_available_indices(\"Europe\")\n",
    "def compare_global_to_local(factor=\"Value\"):\n",
    "    euro_df = indexHandler.get_historic_stock_data(european_indices[factor+\" (Europe)\"][\"code\"])\n",
    "    euro_df = euro_df.rename(columns={\"level_eod\":factor+\" (Europe)\"})\n",
    "   \n",
    "    comparison_df = euro_df.merge(df[[factor,\"MSCI World\",\"date\"]],how='inner',left_on=\"date\",right_on=\"date\")\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    for k in [factor,factor+\" (Europe)\",\"MSCI World\"]:\n",
    "        plt.plot(comparison_df['date'],comparison_df[k],label=k)\n",
    "    plt.legend(prop={'size':12})\n",
    "    plt.xlabel('date', fontsize=18)\n",
    "    plt.ylabel('', fontsize=18)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-humanitarian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_global_to_local(\"Momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_global_to_local(\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_global_to_local(\"Small-Cap (Value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-orientation",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "What we see is that the european index is in all 3 cases worse than the Global one (or in the Small-Cap one, the european one).\n",
    "The value one basically gets outperformed like crazy since the financial crises in 2008. But then again, thats when Value got hit hard.\n",
    "Momentum went through the roof since 2015ish in the world variant but not in europe. Instead there was barely any gain!\n",
    "\n",
    "The Small-Cap (Value) one may be the most intersting one to diversify. Here the EU one even outperformed the US one pre financial crises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-browser",
   "metadata": {},
   "source": [
    "## Emerging Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_indices = indexHandler.get_available_indices(\"EM\")\n",
    "def compare_em_to_local(factor=\"Value\"):\n",
    "    em_df = indexHandler.get_historic_stock_data(em_indices[factor+\" (EM)\"][\"code\"])\n",
    "    em_df = em_df.rename(columns={\"level_eod\":factor+\" (EM)\"})\n",
    "   \n",
    "    comparison_df = em_df.merge(df[[factor,\"MSCI World\",\"date\"]],how='inner',left_on=\"date\",right_on=\"date\")\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    for k in [factor,factor+\" (EM)\",\"MSCI World\"]:\n",
    "        plt.plot(comparison_df['date'],comparison_df[k],label=k)\n",
    "    plt.legend(prop={'size':12})\n",
    "    plt.xlabel('date', fontsize=18)\n",
    "    plt.ylabel('', fontsize=18)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_em_to_local(\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_em_to_local(\"Momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_em_to_local(\"Small-Cap (Value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-secret",
   "metadata": {},
   "source": [
    "# Known Drawbacks - Limitations and FAQ\n",
    "\n",
    "## The data is only back to 1998\n",
    "\n",
    "## Isn't Value dead?\n",
    "\n",
    "## Value has a lot of crap in it\n",
    "\n",
    "## Small Cap Value's highest holding is GME - isn't that crazy?\n",
    "\n",
    "\n",
    "## There is only this single SPDR Small Cap Value ETF - isn't it good?\n",
    "\n",
    "## What about High Dividends?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-change",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-brief",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
